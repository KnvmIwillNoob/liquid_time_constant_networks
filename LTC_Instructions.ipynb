{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Liquid Time-Constant Neural Network (LTNN) Implementation\n",
        "*Implemented by Michael B.Khani*\n",
        "## Introduction\n",
        "Liquid Time-Constant Neural Networks (LTNN) are a type of neural network designed to learn complex functions. Their primary characteristic is the unique processing in the hidden layer which relies on prior activations.\n",
        "\n",
        "In this tutorial, we'll implement an LTNN to approximate the linear function y=2x+1. Our network will comprise of three layers:\n",
        "1.   Input layer\n",
        "2.   Hidden layer (with unique LTNN processing)\n",
        "3.   Output layer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j3Lv-LGzoPsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Problem Definition\n",
        "We aim to approximate the function y=2x+1 using a set of input-output pairs as our training data. The goal is to fine-tune the network's parameters such that its predictions are as close as possible to the true outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "3STEC2ouopfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Preparation\n",
        "To approximate our target function, we'll generate training data for the first five integers (0 to 4):"
      ],
      "metadata": {
        "id": "wppMr6oHo_gx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iObpGp9ooHsu"
      },
      "outputs": [],
      "source": [
        "#training_data = [(i, 2*i + 1) for i in range(5)]\n",
        "training_data = [(0, 1), (1, 3), (2, 5), (3, 7), (4, 9)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Network Architecture\n",
        "1. Input Layer:\n",
        "*   Single node for the input value I_0.\n",
        "2. Hidden Layer:\n",
        "*   Contains two nodes:\n",
        "First node (x_0) always set to 1.0, acting as a bias.\n",
        "Second node (x_1), processing input combined with the prior activation (x_0) using weights in theta and a constant A.\n",
        "3. Output Layer:\n",
        "*   A single node that produces the network's output. It computes the result using activations from the hidden layer combined with weight w and bias b."
      ],
      "metadata": {
        "id": "0EwErbwepC3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Initialization\n",
        "Before training, we must initialize the network parameters:"
      ],
      "metadata": {
        "id": "FPmtM6dhp7Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # NumPy is a fundamental package for scientific computing with Python\n",
        "\n",
        "theta = np.array([0.5, -0.2])\n",
        "A = np.array([0.3])\n",
        "tau = 1.0\n",
        "w = 0.4\n",
        "b = -0.1\n",
        "learning_rate = 0.1\n"
      ],
      "metadata": {
        "id": "tOUsVPG5p8mY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Forward Propagation\n",
        "The `forward_pass` function is used to compute the network's output for a given input. It processes data through the layers, applying appropriate weights and biases."
      ],
      "metadata": {
        "id": "w9obs0tDp_9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(input_value):\n",
        "    I_0 = input_value\n",
        "    x_0 = 1.0  # Use 1.0 instead of 0.0 to avoid vanishing activations initially\n",
        "    S_0 = I_0 * theta[0] * A * x_0 + x_0 * theta[1] * A * x_0\n",
        "    dx_dt_0 = x_0/tau + S_0\n",
        "    x_1 = x_0 + dx_dt_0\n",
        "    y_prime = x_1 * w + b\n",
        "    return y_prime"
      ],
      "metadata": {
        "id": "gXyJrMqpqG2-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Loss Calculation\n",
        "We measure how close the network's predictions are to the true outputs using the Mean Squared Error (MSE) loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "gBdrAhwqqLja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(y_prime, desired_output):\n",
        "    return (y_prime - desired_output) ** 2"
      ],
      "metadata": {
        "id": "LyCXisDPqK8-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Backward Propagation\n",
        "To optimize the network, we need to adjust its parameters based on the error it makes. The `backward_pass` function computes the gradients required to adjust the weights and biases."
      ],
      "metadata": {
        "id": "tyQ7bbnoqRca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(y_prime, desired_output, input_value, x_t):\n",
        "    dL_dy_prime = 2 * (y_prime - desired_output)\n",
        "    dL_dw = dL_dy_prime * x_t\n",
        "    dL_db = dL_dy_prime\n",
        "    dL_dtheta_0 = dL_dy_prime * w * input_value\n",
        "    dL_dtheta_1 = dL_dy_prime * w * x_t\n",
        "\n",
        "    return dL_dw, dL_db, dL_dtheta_0, dL_dtheta_1"
      ],
      "metadata": {
        "id": "7aMan7iBqVpA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Accuracy Evaluation\n",
        "Accuracy quantifies how well our model is performing. We want to know the percentage of predictions that are within a 5% margin of the true outputs:"
      ],
      "metadata": {
        "id": "5Hx9CUXjqRAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy():\n",
        "    accurate_data_points = 0\n",
        "    for input_value, desired_output in training_data:\n",
        "        y_prime = forward_pass(input_value)\n",
        "        percentage_error = abs((y_prime - desired_output) / desired_output) * 100\n",
        "        if percentage_error < 5:\n",
        "            accurate_data_points += 1\n",
        "    return (accurate_data_points / len(training_data)) * 100"
      ],
      "metadata": {
        "id": "-6nRdnoMqe1k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Training the LTC NN\n",
        "Now, we'll iteratively adjust our network's parameters using the training data until our model reaches an accuracy of over 95%:"
      ],
      "metadata": {
        "id": "0IrsHlsmqhaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "while calculate_accuracy() < 95:\n",
        "    total_loss = 0\n",
        "    for input_value, desired_output in training_data:\n",
        "        y_prime = forward_pass(input_value)\n",
        "        loss = calculate_loss(y_prime, desired_output)\n",
        "        total_loss += loss\n",
        "\n",
        "        dL_dw, dL_db, dL_dtheta_0, dL_dtheta_1 = backward_pass(y_prime, desired_output, input_value, 1.0)  # x_t=1.0\n",
        "\n",
        "        w -= learning_rate * dL_dw\n",
        "        b -= learning_rate * dL_db\n",
        "        theta[0] -= learning_rate * dL_dtheta_0\n",
        "        theta[1] -= learning_rate * dL_dtheta_1\n",
        "\n",
        "    print(f\"Loss = {total_loss}, Accuracy = {calculate_accuracy()}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmfVG-kCqnRC",
        "outputId": "62fe7a17-5bae-4496-eb63-ed5ade3087ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss = [12.78858314], Accuracy = 0.0%\n",
            "Loss = [13.75772574], Accuracy = 20.0%\n",
            "Loss = [10.34141747], Accuracy = 20.0%\n",
            "Loss = [6.91020373], Accuracy = 20.0%\n",
            "Loss = [4.72203731], Accuracy = 20.0%\n",
            "Loss = [3.38628261], Accuracy = 20.0%\n",
            "Loss = [2.53838222], Accuracy = 20.0%\n",
            "Loss = [1.97054546], Accuracy = 0.0%\n",
            "Loss = [1.57153825], Accuracy = 0.0%\n",
            "Loss = [1.27979247], Accuracy = 0.0%\n",
            "Loss = [1.05945549], Accuracy = 0.0%\n",
            "Loss = [0.8885917], Accuracy = 0.0%\n",
            "Loss = [0.75317769], Accuracy = 0.0%\n",
            "Loss = [0.64390038], Accuracy = 0.0%\n",
            "Loss = [0.55436818], Accuracy = 0.0%\n",
            "Loss = [0.48006781], Accuracy = 0.0%\n",
            "Loss = [0.41773239], Accuracy = 0.0%\n",
            "Loss = [0.36494519], Accuracy = 0.0%\n",
            "Loss = [0.31988387], Accuracy = 0.0%\n",
            "Loss = [0.28115062], Accuracy = 0.0%\n",
            "Loss = [0.24765696], Accuracy = 0.0%\n",
            "Loss = [0.21854358], Accuracy = 0.0%\n",
            "Loss = [0.19312381], Accuracy = 0.0%\n",
            "Loss = [0.17084275], Accuracy = 20.0%\n",
            "Loss = [0.15124751], Accuracy = 20.0%\n",
            "Loss = [0.133965], Accuracy = 20.0%\n",
            "Loss = [0.11868528], Accuracy = 20.0%\n",
            "Loss = [0.10514887], Accuracy = 20.0%\n",
            "Loss = [0.09313696], Accuracy = 40.0%\n",
            "Loss = [0.08246381], Accuracy = 40.0%\n",
            "Loss = [0.07297073], Accuracy = 40.0%\n",
            "Loss = [0.06452133], Accuracy = 40.0%\n",
            "Loss = [0.05699775], Accuracy = 60.0%\n",
            "Loss = [0.05029759], Accuracy = 60.0%\n",
            "Loss = [0.04433138], Accuracy = 60.0%\n",
            "Loss = [0.03902061], Accuracy = 60.0%\n",
            "Loss = [0.03429607], Accuracy = 60.0%\n",
            "Loss = [0.03009644], Accuracy = 80.0%\n",
            "Loss = [0.02636718], Accuracy = 80.0%\n",
            "Loss = [0.02305961], Accuracy = 80.0%\n",
            "Loss = [0.02013014], Accuracy = 80.0%\n",
            "Loss = [0.01753958], Accuracy = 80.0%\n",
            "Loss = [0.01525264], Accuracy = 80.0%\n",
            "Loss = [0.01323746], Accuracy = 80.0%\n",
            "Loss = [0.01146525], Accuracy = 80.0%\n",
            "Loss = [0.00990994], Accuracy = 80.0%\n",
            "Loss = [0.00854795], Accuracy = 80.0%\n",
            "Loss = [0.00735791], Accuracy = 80.0%\n",
            "Loss = [0.00632051], Accuracy = 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Testing the LTNN\n",
        "Finally, once the network is trained, we can use it to make predictions and compare them to the actual outputs:\n",
        "\n"
      ],
      "metadata": {
        "id": "y7fYnw4vqrid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "print(f\"\\nTraining Data: {training_data}\")\n",
        "for input_value, _ in training_data:\n",
        "    print(f\"Input: {input_value}, Predicted Output: {forward_pass(input_value)}, Actual Output: {2*(input_value)+1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUicQVJzqxBS",
        "outputId": "4f94fe6d-2333-4924-ce6d-365502f33242"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Data: [(0, 1), (1, 3), (2, 5), (3, 7), (4, 9)]\n",
            "Input: 0, Predicted Output: [1.04937744], Actual Output: 1\n",
            "Input: 1, Predicted Output: [3.06861444], Actual Output: 3\n",
            "Input: 2, Predicted Output: [5.08785144], Actual Output: 5\n",
            "Input: 3, Predicted Output: [7.10708844], Actual Output: 7\n",
            "Input: 4, Predicted Output: [9.12632544], Actual Output: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "You've now implemented a simple Liquid Time-Constant Neural Network (LTC NN) from scratch and trained it to approximate a linear function. While this example is elementary, the principles applied here can be expanded for more complex tasks.\n",
        "\n",
        "\n",
        "---\n",
        "> Note: Liquid Time-constant Neural networks (LTC NNs) are an interesting architecture that has the advantage of being relatively insensitive to gap length, compared to other RNNs, hidden Markov models, and other sequence learning methods1. While LTC NNs may not be as common as other architectures like CNNs or RNNs, it is always important to consider the nature and requirements of the problem at hand before choosing a suitable neural network architecture.\n",
        "---\n",
        "\n",
        "\n",
        "To get a full understanding, run the provided Python code and analyze the outputs. Experiment by changing parameters, training data, or network structure to gain deeper insights."
      ],
      "metadata": {
        "id": "W4E1TRb8rIdL"
      }
    }
  ]
}